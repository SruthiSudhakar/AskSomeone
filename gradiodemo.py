import gradio as gr
from torch import cuda
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from pinecone import Pinecone
from pinecone import ServerlessSpec
from datasets import load_dataset
from torch import cuda, bfloat16
import transformers
from langchain.llms import HuggingFacePipeline
from langchain.vectorstores import Pinecone as LCVSPinecone
from langchain.chains import RetrievalQA

# Setup
embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'
device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'
embed_model = HuggingFaceEmbeddings(
    model_name=embed_model_id,
    model_kwargs={'device': device},
    encode_kwargs={'device': device, 'batch_size': 32}
)

# Pinecone API Key and Index Configuration
api_key = '4b46f5d6-ba2e-4fc7-950d-c64897e4ed02'
pc = Pinecone(api_key=api_key)
cloud = 'aws'
region = 'us-east-1'
spec = ServerlessSpec(cloud=cloud, region=region)
index_name = 'llama-2-rag'

# Create or connect to Pinecone Index
if index_name not in pc.list_indexes().names():
    pc.create_index(
        index_name,
        dimension=len(embed_model.embed_documents(["test"])[0]),
        metric='cosine',
        spec=spec
    )
index = pc.Index(index_name)

# Load data
data = load_dataset('ss6638/AppajiSpeeches', split='train')
data = data.to_pandas()
batch_size = 32
for i in range(0, len(data), batch_size):
    batch = data.iloc[i:i+batch_size]
    ids = [f"{x['discourse_date']}" for _, x in batch.iterrows()]
    texts = [x['discourse_chunk'] for _, x in batch.iterrows()]
    embeds = embed_model.embed_documents(texts)
    metadata = [{'text': x['discourse_chunk'], 'dates': x['discourse_date']} for _, x in batch.iterrows()]
    index.upsert(vectors=zip(ids, embeds, metadata))

# Load Model
model_id = 'meta-llama/Llama-2-70b-chat-hf'
hf_auth = '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'
bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=bfloat16
)
model_config = transformers.AutoConfig.from_pretrained(
    model_id,
    use_auth_token=hf_auth
)
model = transformers.AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    config=model_config,
    quantization_config=bnb_config,
    device_map='auto',
    use_auth_token=hf_auth
)
tokenizer = transformers.AutoTokenizer.from_pretrained(
    model_id,
    use_auth_token=hf_auth
)
generate_text = transformers.pipeline(
    model=model, tokenizer=tokenizer,
    return_full_text=True,
    task='text-generation',
    temperature=0.0,
    max_new_tokens=512,
    repetition_penalty=1.1
)

llm = HuggingFacePipeline(pipeline=generate_text)

# Initialize RetrievalQA Chain
text_field = 'text'
vectorstore = LCVSPinecone(
    index, embed_model.embed_query, text_field
)
rag_pipeline = RetrievalQA.from_chain_type(
    llm=llm, chain_type='stuff',
    retriever=vectorstore.as_retriever()
)

# Gradio Interface Function
def generate_answer(prompt):
    res = llm(prompt)
    ragres = rag_pipeline(prompt)
    return f"LLM Response: {res[0]['generated_text']}\n\nRAG Response: {ragres['result']}"

# Gradio Interface
interface = gr.Interface(
    fn=generate_answer,
    inputs=gr.inputs.Textbox(lines=2, placeholder="Enter your prompt here..."),
    outputs="text",
    title="LLaMA-2 RAG Demo",
    description="Ask a question and get responses generated by LLaMA-2 with RAG."
)

if __name__ == "__main__":
    interface.launch()
